% NOTE TO AIP TYPSETERS: TO CONVERT FROM TWO-COL TO PREPRINT, SWITCH
% COMMENTOUT COMMAND FROM A TO B IE. USE:
%
%%\documentclass[prl,twocolumn,showpacs,twocolumngrid,superbib]{revtex4}
%\documentclass[prl,twocolumn,twocolumngrid,superbib]{revtex4}
%\newcommand{\commentoutB}[1]{}
%\newcommand{\commentoutA}[1]{#1}
%
% INSTEAD OF THE FOLLOWING:
%
\newcommand{\commentoutB}[1]{#1}
\newcommand{\commentoutA}[1]{}
%\documentclass[prl,aps,preprint,showpacs,superbib]{revtex4}
\documentclass[prl,aps,preprint,superbib,12pt]{revtex4}

\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{alltt}
\usepackage{fancyhdr}
\newcommand{\bms}[1]{{\boldsymbol #1}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

%\draft
%\tighten
\pagestyle{fancy}

\begin{document}

\title{On the recognition of internal coordinates in various 
chemical systems}

\author{K\'aroly N\'emeth\footnotemark[1]}
\author{Matt Challacombe}

\affiliation{Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM 87545, USA}

\date{\today}

\begin{abstract}
This paper presents several considerations for the appropriate choice 
of internal coordinates in various chemical systems. The appropriate
and black box recognition of internal coordinates is of fundamental 
importance
for the extension of internal coordinate algorithms to all fields where
previously Cartesian coordinates were the preferred means of geometry 
manipulations. Such fields range from local and global geometry 
optimizations to molecular dynamics as applied to a wide variety of
chemical systems. We peresent a robust algorithm that is capable 
to quickly determine the appropriate choice of internal coordinates
in a wide range of atomic arrangements.
\end{abstract}

%\pacs{31.15.-p,31.15.Ne,02.60.Pn, 45.10.Db, 02.40.Hw} 

\maketitle

\footnotetext[1]{\tt KNemeth@LANL.Gov}

\section{Introduction}
The earliest use of curvilinear internal coordinates 
in computational chemistry
is associated with vibrational analysis \cite{EWilson55}. 
It has been recognized in vibrational analysis 
that most molecular vibrations
are fairly well localized on internal coordinates that reflect
chemical concepts, such as chemical bonds, valence angles or
dihedral torsions. In fact, vibrational coupling between
appropriately chosen internal coordinates is typically
an order of magnitude smaller than in the corresponding Cartesian
representation \cite{PPulay69,GFogarasi79,GFogarasi92,PPulay77}.
This observation holds not only for harmonic but more importantly
also for anharmonic vibrational couplings.
The recognition of this reduced vibrational coupling lead to the
development of internal coordinates based geometry optimization
\cite{PPulay69,PPulay77,GFogarasi79,HSchlegel82,HSchlegel03} which
is now the standard mean of local optimization in most quantum
chemistry software packages. Internal coordinate geometry optimization
reduces the number of optimization steps typically by a factor of 2-10
for small or medium sized molecules, as compared to Cartesian
conjugate gradient algorithms \cite{TBucko05}. Another field of
potentially great use of internal coordinates is represented
by molecular dynamics \cite{PPulay02}, where research aims on developing
efficient simulation of long time-scale dynamics of large molecules.

In order to define an appropriate internal coordinate set, first
the molecular connectivity has to be determined.
The connectivity of atoms is usually recognized on the basis of 
overlapping spheres of atomic or Van der Waals radii 
\cite{VBakken02,TBucko05,KNemeth04}. Atomic radii are often not suitable
to recognize bonding in e.g. ionic salts, since typical ionic radii
may be 2-3 times larger or smaller then atomic ones 
\cite{Slater_64v41}. The application
of Van der Waals radii usually results in a huge number of 
connectivities of which most ones are neither chemically relevant nor 
helpful for the optimization. In fact an overly large number
of internal coordinates can substantially decrease the efficiency of
an internal coordinate optimizer. On the other hand, missing internal 
coordinates can lead to inefficient or non-convergent optimizations.

Besides the case of ionic systems or random clusters of atoms,
also fragmented sytems can cause problems. Consider, for example
a system of isolated molecules that interact only by very long range
forces but the intermolecular interaction cannot be described
in terms of well defined bonds such as in covalently bound systems.
E.g. two water molecules at a larger separation and at a random 
orientation, where no clear hydrogen bonding exists but there is still
a dipole-dipole interaction.
In principle the interaction of these isolated units could be treated
in terms of Cartesian coordinates of the correspondig centers 
of masses
and the appropriate rotations around them, however use of 
relative (internal) coordinates could provide a substantial advantage
in the description of the relative motions of these units.

Another very important area where internal coordinate recognition
may become troblesome is related to Van der Waals contacts.
Consider for example two strands of polymeric teflon. Which 
Van der Waals contacts should we allow for the optimization of their
structures and which ones not? The same question raises even more
importantly in biopolymers, such as proteins, when one considers
their interchain interactions. Or, also in many important
inorganic systems, e.g. in sulphur crystal where Van der Waals 
interaction determines the size and shape of the elementary cell and
may also have a considerable influence on the structure of the
covalently bound S$_{8}$ rings.

Clearly, an important disadvantage of internal coordinates 
over Cartesian ones
is that while Cartesian coordinates are always readily and
obviously at hand,
internal ones are often somewhat arbitrarily defined and thus
can provide a large, definitions-based performance difference
in geometry optimization. 
Unfortunately,
a general algorithm that would overcome this inconvenience in the use
of internal coordinates has not yet been developed. This is probably
the reason why internal coordinates based geometry manipulation
is sometimes cosidered an art rather then science.
Indeed, the problem is that in many of the
problematic systems the boundaries of the chemical concepts are 
reached while the universality of Cartesian coordinates is always 
clearly undoubted. Thus, at one hand internal coordinates offer
great advantage in geometry manipulations on the other hand their
use seems to be much less deterministic then the one of Cartesians.
In the present paper we attempt to bridge the coordinate recognition
gap between the great performance of internal coordinates and the 
troblesomeness of their definition.

\section{Rigidity and flexibility of internal coordinate systems}
Let us suppose that a highly redundant set of primitive
internal coordinates (stretches, bends, torsions, out-of-planes
and linear bendings) is
at hand and we want to reduce this set 
by selecting out the minimally necessary subset of 
primitive internal coordinates that is sufficient for succesful
geometry optimization.
On one hand, reducing the number of primitive internal coordinates
is important because this reduces the rigidity of the internal 
coordinate system and allowes for faster optimizations.
On the other hand, an internal coordinate system that represents
overly large flexibility may also be dangerous for the optimization.
For example if a folded protein would be described only by the covalent
bonds related coordinates, topologically far but spacially close
pieces of the polymer would crash into each other during the 
optimization. Thus a reasonable balance between rigidity and
flexibility must be found for a good internal coordinate system.

The rigidity of an internal coordinate system appears when
one wants to turn arbitrary internal coordinate displacements
into a displaced Cartesian structure, for example
during the course of the iterative back-transformation 
\cite{PPulay77} in a geometry optimization step. 
We define the rigidity of the internal coordinate system, $r$, 
as the norm of the difference between the desired internal coordinate 
displacements $\Delta \phi_{d}$ 
and the realized displacements $\Delta \phi_{r}$: 
\begin{equation}
r = \| \Delta \phi_{d} - \Delta \phi_{r} \| .
\end{equation}
The fundamental reason for the appearence of non-zero
rigidity in optimization steps is 
that it is impossible to predict exact, $r=0$ curvilinear
steps by the current means of geometry optimization, unless
the step is small enough or the coordinate system has a flexibility 
similar to that of the z-matrix.

Overly large flexibility can also manifest itself through the iterative
back-transformation. For example if a protein is optimized
using only its covalent-bond skeleton, an iterative 
back-transformation can easily put
topologically far but spacially close atoms in an strongly 
overlapping position, that in turn results in huge forces.



One important technique 
that reduces the rigidity in an optimization step
is based on a projection method \cite{PPulay92} that filters out
so-called first order redundancies, while usually reduces 
the step-size.

One can also investigate the covariance matrix $R$
of the rigidity vector $\Delta r = \Delta \phi_{d} - \Delta \phi_{r}$,
\begin{equation}
R = \Delta r \Delta r^{t} ,
\end{equation}
where all vectors denote column vectors. This covariance matrix,
as summed up over several optimization steps and diagonalized 
can provide linear
combinations of internal coordinates that minimize the rigidity.
A similar idea \cite{MKarplus81,AStrachan04} 
is already widely used in the construction of 
effective normal modes, e.g. on the basis of correlated velocies
(instead of $\Delta r$-s) where velocity vectors
are provided by molecular dynamics steps. Also note that the so-called
error-matrix of the geometric DIIS algorithm \cite{PPulay84} is also a
covariance matrix and its diagonalization can provide effective
normal modes.

The theory of rigidity of internal coordinate systems has 
also been discussed recently in the context of the flexibility
analysis of protein regions \cite{AJRader02}.

In the following we describe several important conditions 
that can assist one to select an internal coordinate set 
for stuctural manipulations of large molecules.
The main driving force of all the present investigation is to study the
ways the rigidity of an internal coordinate system can be minimized.

\subsection{Selection based on condition number}
The most important computational operations 
that are carried out in geometry optimization 
with internal coordinates are the coordinate transformations 
\cite{PPulay77}. The coordinate transformations involve
Wilson's $B$ matrix \cite{EWilson55}, $B$, and its pseudo-inverse, $A$.
Wilson's $B$ matrix is defined as
\begin{equation}
B_{ij} = \frac{\partial \phi_{i}}{\partial x_{j}},
\end{equation}
where $\phi_{i}$ denotes the $i$-th internal coordinate and $x_{j}$
denotes the $j$-th Cartesian coordinate of the molecule.
The matrix $G_{c}$ connects the $B$ matrix with its pseudo-inverse
$A$ via the definitions
\begin{equation}
G_{c} = B^{t} B
\end{equation}
and
\begin{equation}
A = G_{c}^{-1} B^{t} ,
\end{equation}
where $G_{c}$ is an $N_{c} \times N_{c}$ matrix, with $N_{c}$ being the
number of Cartesian coordinates, and $N_{c}=3N$, for a molecule
with $N$ atoms. $G_{c}$ has $3N-6$ nonzero eigenvalues that refer to 
the internal degrees of motion of a non-linear molecule, and
$6$ zero eigenvalues referring to molecular translations and rotations. 
Thus the
matrix, $G_{c}^{-1}$ must be computed as a generalized inverse. 
Let $\lambda_{max}$ and $\lambda_{min}$ denote the maximum and minimum
of the largest $3N-6$ eigenvalues of $G_{c}$ and 
$\lambda_{c}=\lambda_{min}/\lambda_{max}$ is the condition number
of $G_{c}$. 
$\lambda_{c}$ can always be calculated once
an internal coordinate set is given. 
The first condition (Condition \#1) that any internal coordinate set
must satisfy is that the $G_{c}$ matrix calculated over the 
internal coordinates given must have $3N-6$ nonzero eigenvalues. 
This condition refers to the requirement
that the internal coordinates must span the whole space 
of internal motions of the molecule.
Once this first condition is satisfied one can think of making
selection between the actual internal coordinates and search
for a reduced set of internal coordinates that still satisfies 
Condition \#1 with the minimal number of internal coordinates, so that
rigidity can be reduced.
Say, for an $N=100$ atoms molecule $N_{i}=294$ internal coordinates
can be selected that satisfy Condition \#1. However, these $N_{i}=294$
internal coordinates can be selected in lots of different ways,
i.e. lots of sub-sets of the originally $N_{i}>294$ elements internal
coordinate set can be chosen. To make a further selection
among these subsets, Condition \#2 can be introduced:
let us choose the one among equal-sized subsets which provides the
largest $\lambda_{c}$ condition number. This way both
the size and the choice of internal coordinates can be chosen
uniquely, unless degenerate subsets occure in the $\lambda_{c}$ sense.

It is needless to say that the calculation of $\lambda_{c}$
for all possible selections of internal coordinates would be 
very demanding computationally, even if it can be done very efficiently
for each individual coordinate set using sparse matrix techniques.
Thus we consider the $\lambda_{c}$-based
selection procedure impractical for large molecules, even though it
provides a mathematically strict formulation for reducing the rigidity
of a coordinate system.

\subsection{Linear combination of primitive internal coordinates}
The condition number $\lambda_{c}$ of a given internal coordinate set
can also be modified by the construction of appropriate linear 
combinations of primarily given internal coordinates. Usually, the
primary internal coordinate set consists of so-called primitive
internal coordinates, i.e. stretchings, bendings, torsions,
out-of-planes and linear bendings. One well established set of such
linear-combinations is the so-called natural internal coordinates
\cite{GFogarasi92,MvonArnim99}. Unfortunately, the construction
of natural internal coordinates is difficult to 
automatize especially for systems that contain
lots of fused rings \cite{BPaizs00}. 

Alternatively, one can use the so-called delocalized
internal coordinates \cite{JBaker96}. Delocalized internal coordinates
provide a $3N-6$ dimensional set of linear-combined primitive
internal coordinates, where the linear combination coefficients
come from a unitary matrix, $U$. $U$ is constructed either by
diagonalizing the matrix $G_{i}=BB^{t}$ and taking eigenvectors of 
its non-zero subspace or by QR-decomposition of
$B$. Note that these two procedures result in different $U$ unitary 
matrices. The purpose of constracting $U$ is solely to use
the non-zero subspace of $G_{i}$ to get $3N-6$ linear combinations
of primitive internal coordinates. Any rotationally transformed
$U$ can be considered a legitimate linear combination as it still spans
the non-zero subspace of $G_{i}$. In practice 
only the $U$ matrices constructed by diagonalization 
\cite{JBaker96,JAndzelm92} or 
by QR-decomposition (e.g. in Ref. \onlinecite{TBucko05})
are used. The effect of the additional unitary trasnformation 
of $U$ has not been studied yet. Also note that the generation of
the delocalized internal coordinates for very large molecules 
can not be carried out in a robust fashion,
as both diagonalization and QR-decomposition are
inefficient for this purpose. While diagonalization scales cubically
with system size, sparse QR-decomposition can best achieve 
quadratic scaling.

We do not discuss further the applicability of delocalized internal 
coordinates for very large systems since our primary goal here is 
to focus on how to select primitive internals for a given molecule.

\subsection{Selection based on Cholesky factorization}
Paizs {\it et.al.} describe a technique that is based
on Cholesky factorization, to determine the $3N-6$ most independent
coordinates of a redundant internal coordinate set \cite{BPaizs00}.
The technique can be viewed as a sparse Cholesky factorization
with column pivoting (see e.g. Ref. \onlinecite{GGolub96}). 
Such a technique
is tipically used to determine trapezoidal (incomplete) Cholesky
factors of positiv semidefinite matrices, such as the $G_{i}$ matrix
in the above discussions. The idea is based on the use of
pivoting when factorizing the $G_{i}$ matrix. The most independent
$3N-6$ internal coordinates are supposed to have large pivots, 
which makes it possible to select a unique $3N-6$ dimensional
set of coordinates, unless substantial degeneracies occur.

As is well known, the application of pivoting
is a major computational bottleneck when applied in large sparse 
Cholesky factorization \cite{AGeorge81} 
since it results in a huge amount of work
in reorganizing matrices in the memory. For this reason pivoting is 
most often avoided in sparse Cholesky factorizations. Instead,
efficient symmetric permutations are used, such as the
reverse Cuthill-McKee ordering \cite{AGeorge81}, to reduce filling
of the Cholesky factors. Due to the technical difficulties,
pivoting-based coordinate selection does not seem efficient for large 
molecules.

\subsection{The von Armin approach}
I our opinion the best algorithm for practical 
coordinate recognition so far described
in the literature is the one by von Arnim and Ahlrichs 
\cite{MvonArnim99}. Here we only focus on their algorithm of 
recognizing the molecular connectivity. In this approach
atomic radii are gradually increased while overlapping atomic 
spheres are checked until the whole molecule becomes interconnected.   
In the first phase somewhat scaled atomic radii are are used
to recognize the strongest bonds. In the second phase, when 
atomic radii are increased, first hydrogen bonds are checked and 
if fragments are still not connected, bonds between any other atoms
are also allowed. 

The great advantage of the von Armin approach of
connectivity recognition against all previously
mentioned analysis is that it can be carried out in an extremely robust
fashion and does not require much use of large linear algebraic
manipulations, as opposed to e.g. pivoting-based Cholesky 
factorization or $\lambda_{c}$ based coordinate selection methods.

\section{Length-scale scanning}
Our algorithm for the recognition of connectivity is similar
to the one of von Arnim in the sense that it also gradually scannes 
ranges of atomic radii and checks the connectivity, until all fragments
are connected. Although we carry out the same kind of length-scale
scanning, the way we do it is efficiently adapted for use in very large
molecules. Also, the intermediate steps of connectivity recognition
differ from the ones used by von Arnim.  

Our algorithm consists of two loops.

The goal of the first loop is to find the shortest bonds
that build a molecular topology in which the molecule is not 
fragmented. These bonds provide a molecular skeleton, which together
with the corresponding angles (bendings, torsions, out-of-planes and 
linear-bendings) is sufficient to satisfy Condition \#1, i.e.
these coordinates span the whole space of internal motions. 

The goal of the second loop is to recognize those bonds, which 
have not been recognized in the first loop, because they have not been
dominant bonds in their local environment, but still have relevance
for the optimization. These bonds are often the weaker bonds,
like hydrogen-bonds, Van der Waals contacts or ionic
contacts, but they also may be strong covalent bonds.

In both loops the molecule is divided into cubes and
serial numbers of atoms falling in cubes are stored in a sparse matrix
fashion. The linear size of the cubes starts at 3.0 \AA
and will be multiplied by a factor of 1.05 in each new step of the loop
with the purpuse of allowing longer bonds to appear within cubes and 
between neighboring cubes.
Then all possible connectivities are checked within each cube 
and its neighbors. 

In the first loop, all atoms are associated with a serial number of a 
molecular fragment the atom is part of. Only connectivities
between different fragments are checked. Obviously, in the first
step of the first loop every atom belongs to a different fragment.

Initially, atomic radii are set to 0.8 times the Slater radii
\cite{Slater_64v41}. If the atomic spheres overlap and the atoms 
belong to a different fragment, a bond is
introduced. At the end of each cycle the sparse topology
matrix is built on the basis of the bond-list. The 
reverse Cuthill-McKee ordering \cite{AGeorge81} is applied
to the sparse topology matrix which brings the topology matrix
into a block-structured appearence. In this ordering unconnected 
fragments of the molecule appear as isolated blocks in the 
topology matrix. Based on this block structure, atoms are associated
with new serial numbers of fragments they are part of.
If there are no fragments the first loop is exited, otherwise
atomic radii and linear sizes of the cubes are multiplied by 1.05
and a new cycle of the first loop starts. The first loop repeats
itself until the molecule is fragmented. 

For ordinary organic molecules the first loop is normally
enough to recognize all covalent bonds. For molecules, in which 
weak bonds play an important role a second loop is necessary. 
For example in a folded protein the first loop recognizes only
the covalent skeleton. All weak interchain bonds must be added
afterwards. Or in a distorted crystal structure of sodium-chloride
the first loop only recognizes a Z-matrix like stucture while
important ionic contacts may be left uncounted.

Also note, that the first loop

-divide system into cubes of gradually increasing size
-start with small radius, recognize fragments by reordering
the sparse topology matrix
-build sceleton, connect only different fragments until sceleton is ready
-second scan: add h-bonds and other useful coordinates that were not found in first scan, avoid z-matrix like formation, don't connect topologically close atoms
-another scan might be used with gaussian weights

\section{Conclusions} \label{Conclusions}

\bibliography{../../Bib/mondo_new}



\end{document}

